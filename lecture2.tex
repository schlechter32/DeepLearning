\mainsection{4}{Dense Neural Networks}{15/05/2020}

\section{Activation function}
Mild requirements on $ \phi() $: \\
\textbullet nonlinear in general $ \rightarrow $ fundamental \\
\textbullet smooth, differentiable $ \rightarrow $ for training \\
\textbullet simple calculation $ \rightarrow $ low complexity \\
\textbullet Slides 4-6; 4-7 activation function types \\

\subsection{Sigmoid activation function}
$ \phi (a) = \sigma (a) = \dfrac{1}{1 + e^{-a}} $ \\
\includegraphics[width=\linewidth]{Images/SigmoidActivationSketch.png}\\
\textbullet $  0 < \phi < 1 \overset {\hat{}}{=}  $ prob.\\
\textbullet symmetry: $ \phi(-a) = 1-\phi(a)  $ \\
\textbullet derivative: $ \dfrac{d \phi(a) }{da} = ... = \dfrac{e^{-a}}{(1+ e^{-a})^2}  = \phi (a) \cdot \phi (-a ) = \phi (a) (1- \phi (a) ), \in (0,1) $\\
easy calculative \\
\textbullet widely used in conventional NN (shallow) \\
\subsection{hyperbolic tangent activation function }
$ \phi (a) = tanh(a) = \dfrac{e^{a} -e^{-a}  }{e^{a}  + e^{-a} }    $ \\
\includegraphics[width=\linewidth]{Images/HyperbolicTangentActivation.png} \\
 like sigmoid but another output range 
 \subsection{rectifier linear unit(ReLU}
 $  \phi (a ) = ReLU(a) = max (a,0) =  \left\lbrace \begin{array}{lc}
 a & a \geq 0 \\
 0 & a < 0 
 \end{array} \right.$ \\
 \includegraphics[width=\linewidth]{Images/ReLuActivation.png}\\
 \textbullet $ \equiv $ diode \\
 simple calculation \\
 \textbullet $  \dfrac{d \phi}{da} = \left\lbrace \begin{array}{lc}
 1 & a > 0 \\
 0 & a <0 
 \end{array} \right.  = u(a), u(0) = 0$ typically used \\
 \textbullet most popular in DNN \\
 \textbullet Details on 4-7
 \subsection{Softmax activatoin function(classification problem)}
 $  \phi (\underline{a} : \underline{a}  = [a_i ] \in \Re^c \rightarrow \Re^c$ \\
 $  \phi ( \underline{a}) = softmax (\underline{a}) = \left[
 \begin{matrix} 
\  \phi_1 (\underline{a}) \\
  \vdots \\
  \phi_c (\underline{a})
 \end{matrix} \right] $ \\
 $ \phi (\underline{a}) = \dfrac{ a^{a_i}}{\sum_{j=1}^{c} e^{a_i }} , \in (0,1) , \sum_{i=1}^{c} \phi_i (\underline{a}) = 1 $ \\
 \textbullet maps $  \underline{a} \in \Re^c  $ to a categorical PMF with $ c $ classes  \\
 \textbullet $  a_i  $ large$  \rightarrow \phi_i (\underline{a} )  $ close to 1 \\
 \textbullet $  a_i  $ small $ \rightarrow \phi_i (\underline{a} )  $ close to 0 \\
 \textbullet used in the output layer for classification problems \\
 \section{Special case c=2, binary classification problem}:
 $ \phi_1 (\underline{a}) = \dfrac{e^{a_i}}{e^{a_1} + e^{a_2}} = \dfrac{1}{1+ e^{-(a_1 - a_2)}} = \sigma (a_1 - a _ 2) \\
 \phi_2 (\underline{a} = \dfrac{e^{a_2}}{e^{a_1} + e^{a_2}} = 1 - \phi_1 (\underline{a}) = \sigma (a_2 - a_1) \\
 $
 i.e. softmax \\
 \includegraphics[width=\linewidth]{Images/OutputLayerSoftmax.png} \\
  one sigmoid output is sufficient for binary classification instead of 2 output softmax! \\
\textbf{  Derivative of softmax:}\\
  $ \dfrac{ \partial \phi_i (\underline{a}) }{\partial a_j} = ... = \left\lbrace \begin{array}{lc}
\phi:i (\underline{a})\cdot ( 1- \phi_i (\underline{a}) & i=j \\
- phi_i (\underline{a}) \cdot \phi_j (\underline{a}) & i \neq j 
  \end{array} \right. $
  \textbullet 4-9 for details on usage \\
  \section{Chapter 4.5 Universal approximation } 
\includegraphics[width=\linewidth]{Images/Slide410.png}

\subsection{E4.3 Regression with 1 hidden layer}
True function: $ f_0(x)   $\\
Given: $ x(n)  $ and noisy function $ y(n) = f_(x(n)) + z(n) , 1 \leq n \leq N  ,z(n)$ is the noise \\
NN: \\
\includegraphics[width=\linewidth]{Images/E43Architecture.png}\\
i.e. $  x_2 = f(x, \underline{\theta}) = \underline{w}_2^T \sigma ( \underline{w}_1 x + \underline{b}_1 )  $, column times row times scalar \\
$ = \sum_{i=1}^{N_1} w_{2,i} \cdot \underbrace{\sigma ( w_{1,i} x + b_{1,i}) }_{M_1 \text{ nonllinear basis functions of }x} + b_2$ \\
$ \sigma ( w_i x + b_i ) \sigma (w_i ( x + \dfrac{b_i}{w_i})) $ \\
new center at $ \dfrac{-b_i}{w_i} $ and new slope value $ w_i $ \\
Picture is for black 1 $ w_i $ red another $ w_i $ green another example $ w_i $\\
\includegraphics[width=\linewidth]{Images/E43Sketchsigmoid.png}
\section{Chapter 4.4 - 4.6 Loss and cost function}
\textbf{Review chapter 3.4}	\\
\includegraphics[width=\linewidth]{Images/ProbabalisticFrameworkSupervisedLearning.png}\\
$ \underset{\underline{\theta}}{min}  L ( \underline{\theta  }) =  \dfrac{1}{N} = \sum_{n=1}^{N} (l(\underline{x}(n), y(n)_i \underline{\theta}$     cost function for $ d_{train } $ \\
$l(\underline{x}, \underline{y}_j \underline{\theta }) = - ln(q ( \underline{y} | \underline{x}_j \underline{\theta } )  $ loss for one pair $  (\underline{x}, \underline{y})  $ \\
$  q () \leftrightarrow $ DNN ???
\subsubsection{4.6.1 Regression Problem}
$ \underline{x} \in \Re^d $: random input \\
$ \underline{y} \in \Re^c $ desired random output \\
Assumption: DNN estimates the mean of $ \underline{y} $, i.e. \\
$  \underline{y} = f(\underline{x} , \underline{\theta})  + \underline{z} , \underline{z}$ : noise \\
f is calculated by DNN \\
\textbf{case A: } : $  \underline{z} \sim N(\underline{0} , \sigma^2 \underline{\underline{I}}),$ white Gaussian noise\\
$ \underline{y} \sim N(f(\underline{x}, \underline{\theta}), \sigma^2 \underline{\underline{I}}  ) $\\
$ q (\underline{y} | \underline{x} , \underline{\theta}) = \dfrac{1}{(2 \pi \sigma^2)^{c/2}}  exp ( \dfrac{-1}{2 \sigma^2} || \underline{y} - f(\underline{x}, \underline{\theta) ||^2})$ \\
$ l(\underline{x}, \underline{y} , \underline{\theta})  = -ln (q(\underline{y}| \underline{x} , \underline{\theta})) = const.+ \dfrac{1}{\sigma^2} || \underline{y} - f(\underline{x} , \underline{\theta}) ||^2\\
L(\underline{\theta}) = \dfrac{1}{N} \sum_{n=1}^{N} || \underline{y}(n) - f(\underline{x} , \underline{\theta}) ||^2 \\
 \rightarrow l_2- loss, $ mean square error (MSE) loss , least squares (LS) method \\
 min $ L(\underline{\theta}) $ is a parameter estimation problem \\
\textbf{ case B:} \\
$ \underline{z} \sim N(\underline{0} , \underline{\underline{C}}) ,  $ colored Gaussian noise \\
$ L(\underline{\theta})  = \dfrac{1}{N} \sum_{n=1}^{N} [\underline{y} - (f(\underline{x}(n) , \underline{\theta})]^T \cdot \underline{\underline{C}}^{-1} \cdot [\underline{y} - (f(\underline{x}) , \underline{\theta})] $ , \textbf{weighted MSE loss}\\
Rarely used in real life applications: \\
\textbullet how to know $ \underline{\underline{c}} $ \\
\textbullet $ \underline{\underline{C}}^{-1} $ expensive for large $ \underline{\underline{C}} $ \\
\subsubsection{4.6.2 Classification}
$ \underline{x} \ in \Re^d  $: input \\
$ \underline{y} \in \lbrace \underline{e}_1 , \underline{e}_2,..,\underline{e}_c \rbrace $ \\: class label for $ \underline{x} $ in one-hot coding \\
Let $ p_i = P(\underline{y} = \underline{e}i | \underline{x} )  $: true posterior probability \\
ch: 3.2 : $ P(\underline{y} | \underline{x}) = \prod_{i=1}^{c}  p_i ^{y_i}$ true PMF \\
But $ p_i $ unknown \\
\textbf{DNN: }
\textbullet output $ f(\underline{y} ,\underline{\theta}) = [f_i (\underline{x}; \underline{\theta})] \in \Re^c $ an estimate for $  [p_i]  $ \\
\textbullet i. e. $ P(\underline{y} | \underline{x}) $ approximated $ Q (\underline{y} | \underline{x} ; \underline{\theta})  = \prod_{ i=1 }^{c} f_i ( \underline{x} ; \underline{\theta})^{y_i}$\\
in order to ensure : \\
\textbullet $  0 < f_i (\underline{y} ; \underline{\theta}) < 1 \\ $\\
\textbullet $  \sum_{i=1}^{c} f_i (\underline{x} ; \underline{\theta})  = 1,$ \\
softmax is used in the output layer : \\
$  \underline{x} _{L} = f (\underline{x} ; \underline{\theta}) = \phi_L (\underline{a}_lL) = softmax (\underline{a}_L) , $ see ch.4.4\\
$  \rightarrow \text{ Loss } l(\underline{x} , \underline{y} ; \underline{\theta}) = - ln(Q(\underline{y} | \underline{x} ; \underline{\theta })) = \sum_{i=1  }^{c} y_i ln (f_i (\underline{y} ; \underline{\theta}))   = - \underline{y} ^T ln (f(\underline{x} ; \underline{ \theta })) \geq 0 $\\
categorical cross entropy(CE) loss \\
Special case : Binary classification , c = 2 \\
ch.4.4: $ softmax , c = 2 \leftrightarrows sigmoid $ \\
i.e. one output neuron with sigmoid activation function $ f(\underline{x}; \underline{ \theta})  = \sigma (a_L)$ is sufficient\\
Let $  y_1 = y, y_2 = 1 - y ; f_1 = f ; f_2 = 1-f $ \\
$ \rightarrow l(\underline{x} , \underline{y} ; \underline{\theta}) = - y ln(f(\underline{x}; \underline{\theta}) + (1-y)) \cdot ln(1-f(\underline{x}; \underline{\theta})) $ , binary CE loss \\
\textbf{True probabilistic way toward cost functions}\\
\includegraphics[width=\linewidth]{Images/ProbabalisticWayCostFunction.png}\\
\subsection{4.6.3 Semantic segmentation }
\textbf{pixelwise classification }\\
categorical cross entropy loss: \\
$  l(\underline{\underline{X}} , \underline{\underline{Y}}; \underline{\theta}) = \sum_{m=1}^{M} \sum_{n=1}^{N}[- \underline{y}_{mn} \cdot ln (\hat{\underline{y}}_{mn} ( \underline{\underline{X}}, \underline{\theta} )) ] $
sum over all pixels\\
Problem: imbalanced classes \\
e.g (c=2) background and foreground with 90 \% background and 10 \% foreground pixels , 90 \% loss function for the foreground \\
$\rightarrow l(\cdot)$ cares more about the major class and less about the minor class but the minor class(foreground) is object of interest. 
$\rightarrow$ reduced segmentation accuracy for the minor class \\
\textbf{Solutions:} \\
\textbullet Weighted categorical CE loss \\
\textbullet region-based loss 
\textbf{Jaccard index only used for result evaluation not suitable as loss function for training}\\
\textbullet minimize loss, not max J or D for those indexes\\
\textbullet $ |A|  \in \mathds{N}$, not differential with respect to $ \underline{\theta} $\\
\textbullet $ \underline{y}_{mn} $ contains 0 or 1 as desired, but $ \hat{\underline{y}}  $ contains real numbers $ \in (0,1) \in softmax $ \\
$\rightarrow$ adapted definition of J and D are necessary \\
\includegraphics[width=\linewidth]{Images/SoftJaccardDiceLoss.png}\\
\textbf{Very good for strongly imbalanced classes }
\section{Chapter 4.7 Training}
\textbullet training set $ _{train} = \lbrace \underline{x} , \underline{y } , 1 \leq n \leq N \rbrace $\\
\textbullet cost function $ L(\underline{\theta}) = \dfrac{1}{N} \sum_{n=1}^{N} l(\underline{x}(n),\underline{y}; \underline{\theta}) $ \\
\textbullet task: $ min L(\underline{\theta}) $\\
\textbullet optimizer: optimization algorithm to solve the minimization task $ L(\underline{\theta}) $ \\
No closed-form solution! Numerical minimization necessary, see AM (last part) \\
\textbf{in DL: gradient decent approach and variants(like hiking)}\\
need only 1. order derivative of $ L(\underline{\theta}) $ \\
\includegraphics[width = \linewidth]{Images/GradientDescent.png}\\
calculation of the gradient vector $  \underline{\nabla} L(\underline{\theta}) $ is non trivial \\
\subsubsection{Chainrule of derivative (back propagation)} 
$\dfrac{ f(g(\theta))}{d \theta} = \dfrac{df}{dg} \cdot \dfrac{dg}{d \theta} $ \\
Layerindex L: $  \dfrac{\partial L_{cost}(\underline{\theta})}{\partial w_{L,ij}} = \dfrac{\partial L_{cost} }{\partial \underline{x}_{L}} \cdot \dfrac{\partial \underline{x}_K}{\partial \underline{a}_L} \cdot \dfrac{\partial \underline{a}_L }{\partial w_{L,ij}} = \underbrace{ \underline{\underline{J}}_L (\underline{x}_L) \cdot \underline{\underline{J}}_{\underline{x}_L}(\underline{a}_L)}_{\underline{\underline{J}}_L (\underline{a}_L)} \cdot \underline{\underline{J}}_{\underline{a}_L} (w_{L,ij}) $, Jacobi matrices see ch. 3.1\\
Notation: $ \underline{\underline{J}}_y (x ) = \dfrac{\partial y}{\partial x}  $ \\
Layer $ L-1$:\\
$ \dfrac{\partial L (\underline{\theta})}{\partial w _{L-1, ij}} = \dfrac{\partial L}{\partial \underline{a}_L} \cdot \dfrac{\partial \underline{a}_L}{\partial \underline{a}_{L-1}} \cdot \dfrac{\partial  \underline{a}_{L-1}}{\partial w_{L-1,ij}} = \underbrace{ \underline{\underline{J}}_L (\underline{a}_L) \cdot \underline{\underline{J}}_{\underline{a}_L} (\underline{a}_{L-1})}_{\underline{\underline{J}}_L(\underline{a}_{L-1})} \cdot \underline{\underline{J}}_{\underline{a}_{L-1}}(w_{L-1,ij}) $ \\
Layer 1: $ \dfrac{\partial L (\underline{\theta})}{\partial w_{1,ij}} = \underline{\underline{J}}_L ( \underline{a}_L) \cdot \underline{\underline{J}}_{\underline{a}_L}(\underline{a}_{L-1}) , ..., \underline{\underline{J}}_{\underline{a}_2}(\underline{a}_1) \cdot \underline{\underline{J}}_{\underline{a}_1} (w_{1,ij}) $ \\
\includegraphics[width = \linewidth]{Images/Training423.png}\\
\textbf{Calculations for backpropagation on Slide 4-24 to 4-27}\\







	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	