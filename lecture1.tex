

% \section{Title 1}
%
% \subsection{Title 1.1}
% 	You can use the custom putfigure command to add figures to the document. the command takes 6 arguments - how much horizontal size to create a bounding box, width, height, path to image, caption, label.



%\putfigure{1.0}{0.7}{0.3}{Images/rl1}{Many Faces of RL}{fig1_1}

% \begin{align}
% 	\S_t = \f(\H_t)
% \end{align}
% \myequations{Generic state RL system}
\mainsection{1}{Introduction}{03/05/2020}

\section{Overview}
\textbullet p.4 expert talks are not relevant for the exam \\
\putfigure{1.0}{0.7}{0.3}{Images/vlc_ZnQlUU16ty}{Mathematical notations}{fig0_1} \\
\textbullet Element notation (element of vector): $ [x_i = \underline{a} + \underline{b}]_i$

\section{Chapter 1.1 - What is machine learning}
\textbullet Signal processing is not a subset of ML or the other way around, they are identical in the task \\
\textbullet Basic difference is in how to design the processing rule \\
\textbullet Regression means to calculate (SP,ML) a continuous-valued output in the real numbers from a signal, can be one-dimensional \\
\textbullet Classification means to calculate (SP,ML) a discrete-valued output from the natural numbers \\
\textbullet p. 1-5 filter in the middle is the view of a pixel and its 8 neighbours \\
\putfigure{1.0}{0.7}{0.3}{Images/vlc_EyL6r2HUlJ}{Three steps of machine learning}{fig1_1} \\
\textbullet over-fitting just memorizes the training set so test set is needed \\
\textbullet testerrorrate is similar to training error rate $\rightarrow$ no over-fitting \\
\textbullet Test set and training set need to be disjuct concatenated \\
\textbullet TODO: Summary supervised learning and unsupervised learning
\section{Chapter 1.2 - What is deep learning}

\textbullet feature extraction: a feature is a clustered subset of information for recognition \\
\textbullet fish example: fish length, color etc. \\
\qquad \textbullet Needs human experience, can't be calculated \\
\qquad \textbullet DNN solves the problem without feature extraction
\subsection{What is a neural network (NN)}

\textbullet Cascaded pipeline of layers \\
\putfigure{1.0}{0.7}{0.3}{Images/vlc_7mdff7jBZY}{Neural Network}{fig1_1}
\section{Chapter 1.3 - Examples for Deep Learning}
\textbullet Semantic image segmentation is pixelwise classification
\mainsection{2}{Tools for Deep Learning}{02/05/2020}

\section{Datasets}
\textbullet Overview for Training Datasets up to ImageNet they are for teaching
\mainsection{3}{Machine learning basics}{02/05/2020}

\section{Linear Algebra}
\textbullet TODO get Math nicely into the Context book \\
\putfigure{1.0}{0.7}{0.3}{Images/vlc_YAS4jaJ0JB}{Vector Normes}{fig3_1}
\section{Chapter 3.2 Random variable and probability distribution}

\subsection{3.2.1 One random vector}
\putfigure{1.0}{0.7}{0.3}{Images/vlc_ZnQlUU16ty}{One random vector}{fig3_2} \\

PDF for a discrete-valued RV: \\
$p(\underline{x}) = \sum_i p_i \delta (\underline{x} - \underline{x_i}, )$ $\delta (\underline{x}: )$ Dirac function \\
\textbf{cumulative distribution function CCDF} \\
$F(\underline{x}) = \int_{-\infty}^{\infty} p(\underline{z}) d\underline{z}$, \quad $p(\underline{x}) = \frac{\partial^d F(\underline{x}) } {\partial x_i ... \partial x_d}$  \\
\putfigure{1.0}{0.7}{0.3}{Images/vlc_ZnQlUU16ty}{Moments of a vector}{fig3_3} \\


special case d = 1:
$\underline{X} \rightarrow  X \in \mathds{R} \\
\mu \rightarrow \mu \in \mathds{R} \\
\underline{\underline{C}} \rightarrow  $ variance of $ X = Var(X) = \delta^2 = E[(X-\mu)^2] = ... = E(X^2) - \mu^2 \\
\delta = \sqrt{Var(X)} $ : standard deviation \\
For any function $ g (\underline{X} ) $ of $ \underline{X}: E[g(\underline{x})] = \int g(x) \cdot p(\underline{x}) d \underline{x} = (d.v.) \sum_i g(\underline{x}_i \cdot P(\underline{x_i}) ) $ \\
\putfigure{1.0}{0.7}{0.3}{Images/vlc_uBIEApi83B}{Multivariate normal (Gaussian) distribution}{fig3_4} \\
\textbf{one-hot coding : }
Only one bit is 1 e.g. 0100 one cold coding is the inverse \\
Coding for class label, random vector y of length c so the identity matrix with dimension c is used for class labels \\
Reforumulation of the PMF (categorical distribution) by one-hot coding of the classes \\
$\underline{x }  = [x_i] \in \lbrace \underline{e }_1 , \underline{e }_2, .., \underline{e }_c \rbrace $ , i.e. all $ x_i = 0$ except for one single element equal to 1 \\
PMF: $P(\underline{X }=\underline{x }) = P(\underline{x }) = \left\lbrace
\begin{array}{l}
 P_i $ if $ \underline{ x } = \underline{e}_1 $ or $ x_1 = 1 \\
... \\
P_c $ if $ \underline{x } = \underline{ e } _ c $ or $ x_c = 1
\end{array}  \right. = { p_1  }^ {x_1 } \cdot  { p_2  }^ {x_2 } \cdot , ... \cdot { p_c  }^ {x_c } =  \prod _ {i=1 }^ {c } p_i^{x_i } \\
ln(P(\underline{x }) = sum_ { i = 1 } ^c x_i \cdot ln(p_i) = [x_i, ..., x_c ] \cdot \left[
\begin{array} {l}
    ln(P_i) \\
    ... \\
    ln(P_c)
\end{array}  \right] = \underline{x }^T \cdot ln(\underline{P) } $  \\
ln function applied element wise \\
