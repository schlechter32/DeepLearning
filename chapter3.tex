\mainsection{3}{Machine learning basics}{02/05/2020}

\section{Linear Algebra}
\textbullet TODO get Math nicely into the Context book \\
\putfigure{1.0}{1}{0.4}{Images/vlc_YAS4jaJ0JB}{Vector Normes}
\section{Random variable and probability distribution}
\subsection{One random vector}
\putfigure{1.0}{1}{0.5}{Images/vlc_ZnQlUU16ty}{One random vector} \\

PDF for a discrete-valued RV: \\
$p(\underline{x}) = \sum_i p_i \delta (\underline{x} - \underline{x_i}, )$ $\delta (\underline{x}: )$ Dirac function \\
\textbf{cumulative distribution function CCDF} \\
$F(\underline{x}) = \int_{-\infty}^{\infty} p(\underline{z}) d\underline{z}$, \quad $p(\underline{x}) = \frac{\partial^d F(\underline{x}) } {\partial x_i ... \partial x_d}$  \\
\putfigure{1.0}{1}{0.4}{Images/vlc_ZnQlUU16ty}{Moments of a vector} \\


special case d = 1:
$\underline{X} \rightarrow  X \in \R \\
\mu \rightarrow \mu \in \R \\
\underline{\underline{C}} \rightarrow  $ variance of $ X = Var(X) = \delta^2 = E[(X-\mu)^2] = ... = E(X^2) - \mu^2 \\
\delta = \sqrt{Var(X)} $ : standard deviation \\
For any function $ g (\underline{X} ) $ of $ \underline{X}: E[g(\underline{x})] = \int g(x) \cdot p(\underline{x}) d \underline{x} = (d.v.) \sum_i g(\underline{x}_i \cdot P(\underline{x_i}) ) $ \\
\putfigure{1.0}{0.7}{0.4}{Images/vlc_uBIEApi83B}{Multivariate normal (Gaussian) distribution} \\
\textbf{one-hot coding : }
Only one bit is 1 e.g. 0100 one cold coding is the inverse \\
Coding for class label, random vector y of length c so the identity matrix with dimension c is used for class labels \\
Reforumulation of the PMF (categorical distribution) by one-hot coding of the classes \\
$\underline{x }  = [x_i] \in \lbrace \underline{e }_1 , \underline{e }_2, .., \underline{e }_c \rbrace $ , i.e. all $ x_i = 0$ except for one single element equal to 1 \\
PMF: $P(\underline{X }=\underline{x }) = P(\underline{x }) = \left\lbrace
\begin{array}{l}
 P_i $ if $ \underline{ x } = \underline{e}_1 $ or $ x_1 = 1 \\
... \\
P_c $ if $ \underline{x } = \underline{ e } _ c $ or $ x_c = 1
\end{array}  \right. = { p_1  }^ {x_1 } \cdot  { p_2  }^ {x_2 } \cdot , ... \cdot { p_c  }^ {x_c } =  \prod _ {i=1 }^ {c } p_i^{x_i } \\
ln(P(\underline{x }) = sum_ { i = 1 } ^c x_i \cdot ln(p_i) = [x_i, ..., x_c ] \cdot \left[
\begin{array} {l}
    ln(P_i) \\
    ... \\
    ln(P_c)
\end{array}  \right] = \underline{x }^T \cdot ln(\underline{P) } $  \\
ln function applied element wise
\subsection{Multiple random vectors}
\textbullet 3-16 Table for distributions \\
\textbullet product rule for probability
$p(\underline{x } , \underline{ y } ) = p(\underline{x }) \cdot p( \underline{y }) \\$
\textbullet Bayes rule
$ p( \underline{y } | \underline{x } ) = p( \underline{y } | \underline{x } ) \cdot ) \frac{ p(\underline{x }}{ p(\underline{y }}$ \\
\textbullet Independent and identically distributed \\
$ \underline{x} $ and $ \underline{y}$ are independent if: \\
$p (\underline{x}, \underline{y}) = p (\underline{x}) \cdot p(\underline{y}) \leftrightarrow p(\underline{x}| \underline{y}) $ or $ p(\underline{x} | \underline{y}) = p ( \underline{y})$ \\
$ \underline{x}_1 ,... ,\underline{x}_N$ are independent and identically distributed (i.i.d)\\
$P(\underline{x}_1, ..., \underline{x}_N) = \prod_ {i = 1 }^{N } p_i (\underline{x}_i), \underline{X}_i \sim p_i (\underline{x}_i) \\
p_i (\underline{x}_i ) = p(\underline{x}_i) $
$\rightarrow$  $p(\underline{x}_1 , ..., \underline{x}_N) = \prod_{ i=1 } ^{N } p(\underline{x}_i)$ \\
\subsection{Kernel based density estimation }
PDF: $ p(\underline{x}) $ of $ \underline{X} \in \R^d $ unknown , only i.i.d samples $ \underline{x}(n), 1 \leq n \leq N$ \\
kernel-based estimate $ \hat { p } (\underline{x}) of p(\underline{x})$ from $ \underline{x}(n)$ \\
kernel function $ k(\underline{x})$, like a PDF \\
1. $ k(\underline{x}) \geq 0 \forall \underline{x}$ \\
2. $\int k(\underline{x}) d \underline{x} = 1$ \\
$ \hat { p } (\underline{x}) = \frac{1}{N} \sum_ {n=1 } ^N k (\underline{x} - \underline{x} (n)) $ \\
\putfigure{1.0}{1}{0.5}{Images/vlc_5dSX2DXM5y}{Kernel function} \\

\textbf{ Smooth Gaussian Kernel: } \\
$N(\underline{ 0 } , \underline{\underline{I } }) : k(\underline{x}) = \frac{1}{2 \pi ^{ \frac{d}{2}} }  \cdot e ^{ - \frac{1}{2} || \underline{x} ||^2 }$ \\
\textbf{ Dirac Kernel } \\
$ k (\underline{x}) = \delta (\underline{x}) : \text { Dirac function } \\
 \delta(\underline{x}) =  \left\lbrace
\begin{array}{l}
    \infty , \underline{x} = \underline{0 } \\
    0, \underline{x} \neq \underline{0}
\end{array} \right.  \\
 \int \delta  (\underline{x}) d \underline{x} = 1  \\
\text { sampling property } :  \int \delta (\underline{x} - \underline{x} _0 f(\underline{x}) d \underline{x}) = f (\underline{x}_0)$  \\
\textbf{  empirical distribution  } \\
$ \hat { p }(\underline{x}) \cdot   \ \frac{1}{N} \sum _ {n=1 } ^{N } \delta (\underline{x} - \underline{x} (n)) $ \\
\putfigure{1.0}{1}{0.5}{Images/vlc_9TMrsGBi0o}{Estimated PDF function}
\section{Kullback-Leibler divergence and cross entropy }
Dissimilarity measure between 2 distributions: \\
\textbf{Case A: continuous-valued random variables: PDF } \\
$\underline{X} \sim p(\underline{x}): $ true statistical distribution of $ \underline{X} \\
$\qquad $q(\underline{x}) $: approximation for $ p(\underline{x}) $, e.g. by DNN$ \\ $
\textbf{ KL divergence (KLD) between p and q:} \\
$D _{KL } ( p || q) = \int  p(\underline{x}) \cdot ln (\frac{p(\underline{x})}{q(\underline{x})}) d \underline{x} \\
= E_ {\underline{X} \sim p } [ln (\frac{p(\underline{X})}{q(\underline{X})})] $ \\
expectation over $p(\underline{x})$ \\
DKL is real valued scalar positive or negative or 0 \\
\textbf{ Case B: discrete-valued random vector: PMF } \\
$ \underline{X} \in \lbrace \underline{x}_1, ... \underline{x}_c \rbrace \sim $: true PMF of $ \underline{X} \sim Q(\underline{x}) $: approximation for $ P(\underline{x}) \\
D _ {KL } (P|| Q) = \sum _ {i=1  }^ {c} P(\underline{x}_i) \cdot ln (\frac{P(\underline{x})} {Q(\underline{x})}) =
E_{\underline{X} \sim P }  [ ln (\frac{P(\underline{x})}{Q(\underline{x})})] \\
$
Properties of the KL divergence:
P1) Nonnegative $ D _ {KL } (P||Q) \geq 0 \forall p,q \\$
P2) Eguality $ D _ {KL } (P||Q) = 0 $ iff(if and only if) $p(\underline{x}) = q(\underline{x}) \\
$ proof for "sufficient" : $ ln (\frac{p(\underline{x} )}{q (\underline{x})}) = 0 \forall \underline{x} \\
$P1 and P2$ : D_ {KL } (p || 1) $ is a suitable metric for approximation p by q $ \\
$P3 Asymmetry \\
$ D _ {KL } (p||q) = E _ { \underline{X } \sim p} = ln (\frac{p(\underline{x})}{q (\underline{x})}) \neq
D _ {KL } (q||p) = E _ { \underline{X } \sim q} = ln (\frac{q(\underline{x})}{p (\underline{x})}) \\
 $ forward KLD \qquad \qquad \qquad \qquad backward KLD \\
$D _ {KL }$ is not a true distance measure with $D(\underline{x}, \underline{y}) = D(\underline{y}, \underline{x})$ \\
\putfigure{1.0}{1}{0.5}{Images/vlc_aB15OK4H2t}{Forward vs. Backward KL divergence}
\subsection{E3.5 KLD between normal and Laplace distribution }

$ p(x) = \sim N(0, \sigma^2), p(x) = \frac{1}{\sqrt{2 \pi} \sigma} \cdot   e ^ {-\frac{x^2}{2 \sigma^2} } \\
q(x) \sim $ Laplace: $ (0,b) q(x) = \frac{1}{2b} e^{- \frac{|x|}{b}}\\ $
Task: choose b to best approximate p by q. \\
$
\frac{p(x)}{q(x)} = \sqrt{\frac{2}{\pi}} \cdot \frac{b}{\sigma} cdot exp( - \frac{x^2}{2 \sigma^2} + \frac{|x|}{b}) \\
D _ {KL } (p ||q) = E _ { \underline{X } \ sim p} = ln (\frac{p(\underline{x})}{q (\underline{x})}) = ln (\sqrt{\frac{2}{\pi}} \cdot \frac{b}{\sigma}) + E _ {X \sim p } ( ( - \frac{x^2}{2 \sigma^2} + \frac{|x|}{b})) \\
E _ { \underline{X } \ sim p} = \sigma^2 \\
E _ { \underline{X } \ sim p} ( |x|) = \int _ {- \infty } ^{\infty } |x| \cdot \frac{1}{\sqrt{2 \pi} \sigma} exp (- \frac{x^2}{2 \sigma^2}) dx = 2 \int _ {0 } ^{\infty } | \cdot \frac{1}{\sqrt{2 \pi} \sigma} exp (- \frac{x^2}{2 \sigma^2}) dx = \sqrt { \frac{2}{\pi} } \cdot \sigma  \\
$
Let $ \alpha = \frac{\sigma}{b}, D _ {KL } (p ||q) = ...= \sqrt{\frac{2}{\pi}} \cdot  \alpha - ln ( \alpha )+ ln(\sqrt{\frac{2}{\pi}}) - \frac{1}{2}$
$ \frac{d D _ {KL } (p || q)}{d \alpha} = \sqrt{\frac{2}{\pi}} - \frac{1}{\alpha} = 0 \rightarrow \alpha = \sqrt{\frac{2}{\pi}}, $ i.e. $ b \approx 0,86 \\
D _ {KL , min} (p ||q ) = D _ {KL } (p || q ) | \alpha = \sqrt{\frac{2}{\pi}} = .. = \frac{1}{2 } - ln (\frac{\pi}{2}) \approx 0,048 \\
$
\textbullet Probable exam question calculate this for 2 distributions \\
P4) Additive : \\
$
\underline{X} )= (\underline{x}_1, \underline{x}_2), \underline{x}_1 $ and $ \underline{x}_2 $ are independent, $ i.e. \\
p(\underline{x}) ? p_1 (\underline{x}_1) \cdot p_2 (\underline{x}_2),
q(\underline{x}) ? q_1 (\underline{x}_1) \cdot q_2 (\underline{x}_2) \\
$
Then: $ D _ {KL } (p || q) = D _ {KL } (p_1 || q_1)  + D _ {KL } (p_2 || q_2)$ \\
P5) Relation to cross entropy : \\
Definiton of entropy 3-24
probability always greater than 0 but smaller than 1 \\
$
D _ {KL } (p || q) = \int p ln(\frac{p}{q}) d \underline{x} = \int p ln(p) dx - \int p ln(q) dx = - H(p) + H(p,q)  $ or \\
cross entropy: $ H(p,q) = D _ {KL } (p || q) + H(p) \geq H(p) \geq 0$ \\
For a given (fixed) $p(\underline{x}) : H(p)$ fixed \\
\textbf{Hence: } min $D _ {KL } (p || q) \leftrightarrow min H(p,q)$ \\
Not the case for backward KLD $D _ {KL } (q || p)$! \\
Minimizing is not the same anymore because then it is $H(q)$ and thats what we are trying to optimize
\section{Probabilistic framework for machine learning}
valid for both SP and ML \\
valid for both regression problem and classicication problem \\
\putfigure{1.0}{1}{0.4}{Images/vlc_oQFy31Qrgm}{Probabilistic framework of supervised learning} \\
\putfigure{1.0}{1}{0.4}{Images/vlc_yZiUr02XFG}{The data generating distribution} \\
Bayes Rule: \\
\putfigure{1.0}{1}{0.4}{Images/vlc_noaWeuo6ii}{Bayes Rule in DL} \\
\putfigure{1.0}{1}{0.4}{Images/vlc_jGHgEfwvB6}{Bayes decision theorem} \\
\putfigure{1.0}{1}{0.4}{Images/vlc_y3WEcY54hI}{Supervised learning} \\
\textbf{Learning Criterion: } \\
\putfigure{1.0}{1}{0.4}{Images/vlc_cdtgmn3NAF}{Calc part1} \\
\putfigure{1.0}{1}{0.4}{Images/vlc_7J4xeI8M2g}{Calc part2} \\
\subsection{Role of a NN }

1. approximate true posterior $p(\underline{y} | \underline{x}) $ by $ q (\underline{y} | \underline{x} ; \Theta)$ \\
2. learn $\underline{ \Theta }$ from $D_ {train }$