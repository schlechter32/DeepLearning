\mainsection{7}{Convolutional neural networks (CNN)}{15/06/2020}
ch. 4: dense networks \\
\textbullet consisting of dense layers only \\
\textbullet but rarely used. CNN widely used.\\
\textbf{Why?}\\
\textbullet Drawbacks on slide 7-1\\
Solution: CNN, consisitng of many convolutional layers, based on convolution operation\\
\textbullet good for local feature learning\\
\textbullet more efficent, i.e. low complexity
\section{Convlutional layer} 
supports various datatypes:\\
\begin{tabular}{lcc}
input data type:  &$  \X_{l-1}, \A_l ; \X_l$ &$ \W_l$ \\
1D signal waveform & 2D matrix & 3D tensor\\
2D image & 3D tensor & 4D tensor\\
3D data cube (ct image ) & 4D tensor & 5D tensor\\
\end{tabular}\\
One dimension more: multiple input/output signals/images/feature maps/channels\\
e.g. RGB: 3 times 2D images $ \rightarrow $ 3D tensor \\
\textbf{2D convolutional layer l:}\\
Dimension refers to input data shape \\
$ \X_{l-1}  \rightarrow \begin{matrix}
\W_l \\
b_l \\
\phi_l 
\end{matrix} \rightarrow \X_l$\\
\textbullet input tensor $  \X_{l-1} , M_{l-1 }  \times N_{l-1} \times D _{l-1 }$
\\
$ D_{l-1}$(channel depth ) 2D feature maps of size $ M_{l-1} \times N_{l-1 }   $ \\
\textbullet output tensor $  \X_l , M_l \times N_l \times D_l , \X_l = \phi_l (\A_l ) $
$ D_l   $ 2D feature maps of size $  M_l \times N_l $
\textbullet Kernel tensor: $ \W_l , K_l \times K_l \times D_{l-1 } \times D_l  $\\
$ K_l:  $ kernel width:
$ D_l  $ 3D impuls responses/ filters /kernels of the size $  K_l \times K_l \times D_{l-1} $\\
\textbullet bias vector: $  b_l , D_l \times 1  $ \\
one bias for one output channel\\
\textbullet activation tensor: $  \A_l , M_l \times N_l \times D_l  $\\
$ [\A _l ] _{mno} = \sum_{i=1 }^{K_l } \sum_{j=1}^{K_l} \sum_{d=1 }^{D_{l-1}} [\W_l ] _{ijdo} \cdot [\X_{l-1}]_{m+i-1 , n+j-1,d} + [\b_l ]_o$ \\
index (mno, i=1, j=1 , ij, m+i-1, n+j-1 ) spatial correlation, called convolution in CNN \\
index (d=1 , d) sum of all input feature maps\\
index (o,o,o): one of $ D_l  $ output channel\\
$  1 \leq m \leq M_l = M_{l-1 } - K_l + 1  $\\
$  1 \leq n \leq N_l = N_{l-1 } K_l +1  $\\
$  1 \leq o \leq D_l $ \\
 output size reduced by $  K_l -1  $ in each dimension \\
\textbullet $  \phi_l (\cdot):  $ activation function elementwise \\
\putfigure{1.0}{1}{0.4}{Images/CNNLayerVisu}{Visualization of CNN layer} 
\subsection{Properties of convolutional layer}
Typical kernel size $  K_l \times K_l  : 3 \times 3,  5 \times 5 , ... m $ but also $  1 \times 1  $\\
typical number of input channels $  D_l :  $ $  1 \sim 10 \sim 100  $\\
Number of parameters: \\
$ N_{p,l } =\underbrace{ K_l ^2 D_{l-1} D_l}_{\W_l} +\underbrace{ D_l}_{\b_l} \approx K_l ^2 D_{l-1 } D_l $
quite small, independent on input size $  M_{l-1} \times N_{l-1 } $\\
Number of multiplications:\\
$  N_{x,l} = \underbrace{M_l \cdot N_l \cdot D_l}_{ \text { elem in } \A}  \cdot K_l K_l D_{l-1} \approx M_l N_l N_{p,l} $, quite large , depends on input size.\\
\textbullet p1) sparse connection: \\
\putfigure{1.0}{1}{0.4}{Images/IluSparseConnection}{Illusatration sparse connection} \\
small receptive field: $  K_l or K_l \times K_l  \equiv $ focus on local input patterns\\
in DNN:
\putfigure{1.0}{1}{0.4}{Images/receptiveField}{Illustraion receptive field } \\
a neuron in layer l has te receptive field width : $  K_l +, ... ,+ K_l - (l-1) $  for the input layer\\
i.e. a neuron in a deep layer (l $  \uparrow  $) can still be indirectly connected to all input neuron \\
p2) parameter sharing: the same kernel for all output neurons \\
\textbullet p1) + p2) give the following advanteges \\
\textbullet reduced computational complexity reduced memory complexity \\
\textbullet reduced model capacity and reduced overfitting\\
\textbullet p3) translation - equivariant $  \equiv  $ shift-invariant \\
translation /shift of $  \X_{l-1} \rightarrow  $ same translation/shift of $ \X_l $\\
\textbullet p4) one channel/feature-map contains only one feature, e.g. only horizontal edges \\
$ \rightarrow $ need multiple channels/feature maps ($ D_l > 1  $) \\
\textbullet p5) $  M_l = M_{l-1 }  - K_l + 1 :  $ images become smaller.
\section{Modified convolutions}
to:\\
\textbullet reduce complexity \\
\textbullet increase the receptive field\\
\textbullet ... \\
\textbf{Minor modifications }\\
\textbullet Padding slide 7-9 \\
\textbullet Stride slide 7-10  
\textbullet Dilated convolution slide 7-11\\
\textbf{outputsize:} \\
Standard $ M_l = M_{l-1 } - K_l + 1   $\\
including all modifications:\\
$ P \geq 0:  $ padding , 0 \\
$ s \geq 1:  $ stride, 1 \\
$ D\geq 1 :  $ dilation , 1 \\
$ \left\lfloor \dfrac{M_{l-1} + 2P - (K_l -1) D-1}{S} \right\rfloor +1 $\\
Major modifications on $ \W_l $
\textbullet Standard convolution using padding:\\
$  M \times N \times D_{l-1} \underset{\rightarrow}{K \times \times K \times D_{l-1} \times D_l}  M \ times N \times D_l$ \\
3D joint spatial-channnel processing $  \sum_i \sum _j \sum_d  $\\
$ \rightarrow $ high complexity $  N_{x,1} = MNK^2 D_{l-1 } D_l  $
To reduce the complexity:\\
\textbullet $  1 \times 1  $ convolution, i.e. K=1 Slide 7-12\\
\textbullet additional higher nonlinearity due to additional $ \phi(\cdot) $\\
Idea: replace 1 layer of standard convolution by 2 layers of simpler convolutions\\
\textbullet $ M \times N \times D_{l-1 } \overset{1 \times 1 \times D_{l-1 } \times \bar{D} _{l-1 } }{\longrightarrow} M \times N \times \bar{D}_{l-1 } , \bar{D}_{l-1} < D_{l-1 }$\\
\textbullet $  M \times N \times \bar{D}_{l-1 } \overset{K \times K \times \bar{D}_{l-1 } \times D_l}{\longrightarrow} M \times N \times D_l $
$ N_{x,2 } = MN- 1^2 \cdot D_{l-1} + \bar{D}_{l-1 } + MN \cdot K^2 \cdot \bar{D}_{l-1} D_l $\\
$ \rightarrow $ $ \dfrac{N_{x,2}}{N_{x,1}} = \dfrac{D_{l-1}}{K^2 D_l} + \dfrac{\bar{D}_{l-1}}{D_{l-1}} \approx \dfrac{\bar{D}_{l-1}}{D_{l-1}} < 1 $\\
\textbf{\textbullet depth-wise separable convolution:}\\
separable spatial (2D) and channel (1D) processing \\
\textbullet $ M \times N \times D_{l-1} \overset{K \times K \times 1 \times D_{l-1} }{\longrightarrow} M \times N \times D_{l-1}  $ \\
depth wise (2D) convolution for each channel \\
\textbullet $  M \times N \times D_{l-1} \overset {1 \times 1 \times D_{l-1} \times D_L}{\longrightarrow} M \times N \times D_l  $ point wise (1D) convolution = $  1 \times 1  $ convolution \\
$  N_{x,3 } = MNK^2 \cdot 1 \cdot D_{l-1 } + MN \cdot 1^2 \cdot D_{l-1 } D_l  $\\
$  \dfrac{N_{x,3}}{N_{x,1} } = \dfrac{1}{D_l } \dfrac{1}{K_l^2} \approx \dfrac{1}{K_l^2}  $ for $  D_l >> K_l ^2 $\\
\textbullet Comparison Slide 7-13
\section{Pooling and unpooling layer}
2D \textbf{pooling layer} with  stride $  s \in \N $\\
\putfigure{1.0}{1}{0.4}{Images/PoolingLayerVisu}{pooling layer visualization}\\
\textbullet max pooling: $ p = max() $ of a,b,c,...\\
\textbullet mean pooling $ p= mean () $ of a,b,c,...\\
\textbullet l2-norm pooling: $ p= l_2-norm $ of a,b,c,...\\
widely used; 2x2 max pooling with s=2 \\
Nonlinear operation\\
Effects explained on slide 7-15\\
\textbf{2D unpooling layer} e.g. 2x2 \\
Corresponds to upsampling:\\
$ a \rightarrow \left[ 
\begin{matrix}
	a & a\\
	a & a 
\end{matrix}
\right] $\\
upsampling to restore the original image after size pooling
\section{Deconvolutional layer}
deconvolutional layer  $  \neq $ deconvolution, bad name \\
other names= fractionally strided convolution or transposed convolution or learnable upsample\\
\textbf{Goals:}\\
\textbullet replaces unpooling layer \\
\textbullet increase image resolution without changing the object shape in the image \\
\textbf{deconvolutional layer}\\
\textbullet zero insertion \\
\textbullet convolution (smoothing) with learnable kernel\\
\begin{tabular}{ccc}
	&\textbf{ in deep learning} & \textbf{in signal processing}\\
$ \X_{l-1} , \X_l  $ & feature maps & input/output signal\\
$  \W_l  $ & kernel & impulse response \\
 $ \sum_{i} \sum_j w _{ij} x_{m+i, n+j}   $ & convolution & correlation \\
 $  x(n-n_0 ) \rightarrow y (n-n_0) $ & translation-equivariant & shift-invariant \\
 $  x(n-n_0 ) \rightarrow y(n) $ & translation-invariant & - \\
 & padding & zero initialization \\
  & stride & downsampling of output \\
  & dilated convolution & polyphase downsampling of input \\
  reverse convolution & - & deconvolution \\
   & deconvolutional layer & lernable upsampling
\end{tabular}
\section{Flatten layer}
$ \X \in \R^{M \times N \times D}  $ flatten returns long single column $ vec(\X) \in \R^{MND} $\\
as an interface between convolutional layers and dense layers \\
%\putfigure{1.0}{1}{0.4}{Images/file}{Visualization flaten layer}
conv. layer(feature learning) $ \rightarrow $ $\left[ \begin{matrix}
\\
\\
\\
\\
\end{matrix} \right] \rightarrow$  dense layer $ \rightarrow $ output layer (combine and interpret high-level features and make decisions in the output layer)
\section{Global average pooling layer}
\textbf{GAP}: efficient alternative for flatten layer \\
$  \X = [x_{ij}] \in \R^{M \times N \times D} \rightarrow GAP \rightarrow \left[ \dfrac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} x_{ijd} \right]   _{1 \leq d \le D} \in \R^D$\\
\textbullet reduche each feature map/channel to a single value \\
\textbullet as an interface between convolutional and dense layers like the flatten layer\\
\textbullet much less coefficients/parameters for the dense layer after the GAP layer\\
\textbullet suitable for large DNN: reduce complexity
\section{Architecture of CNNs}
A CNN consists of typically \\
\textbullet convolutional layers \\
\textbullet optional max pooling layers \\
both types are used for hierarchical feature learning \\
\textbullet flatten or GAP layer to change the type of layer from convolutional to dense layer to make a decision \\
\textbullet dense layers for final classification or regression \\
\textbullet unpooling or deconvolutional layer is used for image segmentation to get the same output size as the input \\
Examples on slide 7-19 following.\\
Why deep CNN?\\
\textbullet large model capacity for difficult tasks\\
\textbullet multi-level (hierarchical) representation / features of the input image \\
\putfigure{1.0}{1}{0.4}{Images/VisuFeaturesHierarchy}{Visualization deep CNN} \\







