\mainsection{9}{Unsupervised and generative models}{15/06/2020}
\section{Autoencoder(AE)}
a DNN to learn an efficient representation/coding of input data\\
$ \x \rightarrow  \left[\begin{matrix}
encoder, f_E , \\
\underline{\Theta}_E
\end{matrix} \right] \rightarrow \z \rightarrow \left[ \begin{matrix}
decoder, f_D ,\\
 \underline{\Theta}_D
\end{matrix}\right] \rightarrow \hat{\x} \rightarrow \underset{\underset{\y = \x}{\uparrow} }{loss}$\\
$  \x \in \R ^d  $ input \\
$  \z = f_E (\x ; \underline{\Theta}_E) \in \R^c : $ latent variable / representation $  \equiv  $ a hidden code for $ \x  $\\
 $ \hat{\x} = f_D (\z ; \underline{\Theta}_D )= f_D (F_E (\x ; \underline{\Theta}_E ) ; \underline{\Theta}_D )  = f(\x ; \Theta ) \in \R ^d  $ reconstruction for $  \x  $\\
 $  \underline{\Theta} _E ,   \underline{\Theta} _D , = \underline{\Theta} = \left[ \begin{matrix}
 \underline{\Theta} _E \\
 \underline{\Theta} _D 
 \end{matrix}  \right] : $ parameters \\
 $  \y = \x:  $ self-copy, unsupervised ! \\
\textbf{ Encoder /decoder:}\\
\textbullet any NN \\
\textbullet typically symmetric \\
\textbf{Mostly under-complete auto-encode:}: $ c << d  $\\
\putfigure{1.0}{1}{0.4}{Images/UndercompleteAutoencoder}{Undercomplete Autoencoder} \\
overcomplete autoencoder (c >> d) never used \\
Applications of autoencoder:\\
\textbullet a) Dimension reduction like PCA: c << d \\
\textbullet b) Denoising autoencoder \\
$ \x + generated noise / corruptions \rightarrow Autoencoder \rightarrow \hat{\x}\rightarrow \y = \x  $ : $ \rightarrow $ clean input \\
reduce noise / corruption in input\\
$  \z  $: low dimensional ( c<d) \\
$ \rightarrow $ keep only relevant information for $ \x $ , drop noise/corruption information\\
 c) reconstruction-based outlier/anomaly detection\\
\textbullet AE trained on normal data $ \rightarrow  \hat{\x} \approx \x $ for normal data \\
\textbullet if $  \x $ outlier:  $  || \hat{\x} - \x || $ large $ \rightarrow $ outlier can be detected \\
 d) unsupervised preprocessing for other tasks\\
 \textbullet automatic feature learning/extraction $  \z $ instead of manual feature extraction in conventional machine learning
\section{Variational Autoencoder}
Theory of variational autoencoder (VAE):
$ p(\x):  $ unknown distribution of $ \X $,\\
Onlz samples $ \x(1),.., \xi(N) $ available\\
$ q(\xi; \underline{\theta} $: parametric model for $ p(\x) $\\
$ \underline{\theta}: $ parameters of VAE\\
Goal: $ \underset{\underline{\theta}}{min} D_{KL} (p|| q) $\\
As in ch.3.4: 
cost function $ L(\underline{\theta}) = D_{KL} (|| q) = E _{\x \sim p} ln( \dfrac  {p(\X)}  {q(\x ;\underline{\theta})  }) =
\underbrace{E_{\x \sim p} ln(p(\X))}_{\text{independent of } \underline{\theta}}- E_{\x \sim p(\text{unknown) }} ln(q(\x
;\underline{\theta}))$\\
Replace $ p(\x)  $ bz $ \hat{p}(\x)  = \dfrac{1}{N} \sum_{n=1}^{N} \delta (\c - \x_n)$\\
$ \rightarrow L(\underline{\theta}) \approx const - E_{\x \sim p} ln (q(\x ; \underline{\theta})) = const. = \dfrac{1}{N} \sum_{n=1}^{N} - \underbrace{ln (q(\x (n) ; \underline{\theta}) )}_{\text{loss } l(\x_n(n) ; \underline{\theta}) \text{ of VAE}}$\\
Difference to supervised learning in ch. 3.4 and ch. 4\\
\textbullet NO $ \y $ \\
\textbullet $ p(\x ,\y) $ $ \rightarrow  p(\x)$\\
$ q(\y | \x ; \underline{\theta}) $ $ \rightarrow $ $ q(\x_n; \underline{\theta}) $\\
Ho to model $ q(\x_n; \underline{\theta}) $?\\
VAE: $ \x $ generated bz $ \z $ with a known distribution \\
$ \z \sim q(\z) ,  $ e.g. $ \z \sim N(\underline{0}, \I) $\\
$ \underline{\mu}\\  $