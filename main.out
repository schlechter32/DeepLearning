\BOOKMARK [0][-]{Doc-Start}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{What is machine learning}{Doc-Start}% 2
\BOOKMARK [1][-]{section.1.2}{What is deep learning?}{Doc-Start}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{What is a neural network \(NN\)}{section.1.2}% 4
\BOOKMARK [1][-]{section.1.3}{Examples for Deep Learning}{Doc-Start}% 5
\BOOKMARK [0][-]{section.1.3}{Tools for Deep Learning}{}% 6
\BOOKMARK [1][-]{section.2.1}{Software}{section.1.3}% 7
\BOOKMARK [1][-]{section.2.2}{Hardware}{section.1.3}% 8
\BOOKMARK [1][-]{section.2.3}{Datasets}{section.1.3}% 9
\BOOKMARK [0][-]{section.2.3}{Machine learning basics}{}% 10
\BOOKMARK [1][-]{section.3.1}{Linear Algebra}{section.2.3}% 11
\BOOKMARK [1][-]{section.3.2}{Random variable and probability distribution}{section.2.3}% 12
\BOOKMARK [2][-]{subsection.3.2.1}{One random vector}{section.3.2}% 13
\BOOKMARK [2][-]{subsection.3.2.2}{Multiple random vectors}{section.3.2}% 14
\BOOKMARK [2][-]{subsection.3.2.3}{Kernel based density estimation }{section.3.2}% 15
\BOOKMARK [1][-]{section.3.3}{Kullback-Leibler divergence and cross entropy }{section.2.3}% 16
\BOOKMARK [2][-]{subsection.3.3.1}{E3.5 KLD between normal and Laplace distribution }{section.3.3}% 17
\BOOKMARK [1][-]{section.3.4}{Probabilistic framework for machine learning}{section.2.3}% 18
\BOOKMARK [2][-]{subsection.3.4.1}{Role of a NN }{section.3.4}% 19
\BOOKMARK [0][-]{subsection.3.4.1}{Dense Neural Networks }{}% 20
\BOOKMARK [1][-]{section.4.1}{Fully connected neural networks - Neuron}{subsection.3.4.1}% 21
\BOOKMARK [1][-]{section.4.2}{Layer of Nurons}{subsection.3.4.1}% 22
\BOOKMARK [1][-]{section.4.3}{Feedforward neural network}{subsection.3.4.1}% 23
\BOOKMARK [1][-]{section.4.4}{Activation function}{subsection.3.4.1}% 24
\BOOKMARK [2][-]{subsection.4.4.1}{Sigmoid activation function}{section.4.4}% 25
\BOOKMARK [2][-]{subsection.4.4.2}{hyperbolic tangent activation function }{section.4.4}% 26
\BOOKMARK [2][-]{subsection.4.4.3}{rectifier linear unit\(ReLU}{section.4.4}% 27
\BOOKMARK [2][-]{subsection.4.4.4}{Softmax activatoin function\(classification problem\)}{section.4.4}% 28
\BOOKMARK [2][-]{subsection.4.4.5}{Special case c=2, binary classification problem}{section.4.4}% 29
\BOOKMARK [1][-]{section.4.5}{Universal approximation }{subsection.3.4.1}% 30
\BOOKMARK [2][-]{subsection.4.5.1}{E4.3 Regression with 1 hidden layer}{section.4.5}% 31
\BOOKMARK [1][-]{section.4.6}{Loss and cost function}{subsection.3.4.1}% 32
\BOOKMARK [2][-]{subsection.4.6.1}{Regression Problem}{section.4.6}% 33
\BOOKMARK [2][-]{subsection.4.6.2}{Classification}{section.4.6}% 34
\BOOKMARK [2][-]{subsection.4.6.3}{Semantic segmentation }{section.4.6}% 35
\BOOKMARK [1][-]{section.4.7}{Training}{subsection.3.4.1}% 36
\BOOKMARK [2][-]{subsection.4.7.1}{Chainrule of derivative \(back propagation\)}{section.4.7}% 37
\BOOKMARK [1][-]{section.4.8}{Implementation of DNN's in Python}{subsection.3.4.1}% 38
\BOOKMARK [0][-]{section.4.8}{Advanced optimization techniques}{}% 39
\BOOKMARK [1][-]{section.5.1}{Difficulties in optimization }{section.4.8}% 40
\BOOKMARK [2][-]{subsection.5.1.1}{E5.1: sigmoid vs. ReLU}{section.5.1}% 41
\BOOKMARK [1][-]{section.5.2}{Momentum method}{section.4.8}% 42
\BOOKMARK [2][-]{subsection.5.2.1}{Nesterov Momentum}{section.5.2}% 43
\BOOKMARK [1][-]{section.5.3}{Learning rate schedule}{section.4.8}% 44
\BOOKMARK [1][-]{section.5.4}{Input and batch normalization}{section.4.8}% 45
\BOOKMARK [2][-]{subsection.5.4.1}{E5.3 A Perceptron}{section.5.4}% 46
\BOOKMARK [2][-]{subsection.5.4.2}{Batch normalization}{section.5.4}% 47
\BOOKMARK [1][-]{section.5.5}{Parameter initialization}{section.4.8}% 48
\BOOKMARK [1][-]{section.5.6}{Improved \(network\)-model}{section.4.8}% 49
\BOOKMARK [0][-]{figure.5.8}{Overfitting and regularization}{}% 50
\BOOKMARK [1][-]{section.6.1}{Model capacity and overfitting / underfitting}{figure.5.8}% 51
\BOOKMARK [1][-]{section.6.2}{Weight norm penalty}{figure.5.8}% 52
\BOOKMARK [1][-]{section.6.3}{Early stopping}{figure.5.8}% 53
\BOOKMARK [1][-]{section.6.4}{Data augmentation}{figure.5.8}% 54
\BOOKMARK [1][-]{section.6.5}{Ensamble learning}{figure.5.8}% 55
\BOOKMARK [1][-]{section.6.6}{Dropout}{figure.5.8}% 56
\BOOKMARK [1][-]{section.6.7}{Hyperparameter optimization}{figure.5.8}% 57
\BOOKMARK [0][-]{figure.6.7}{Convolutional neural networks \(CNN\)}{}% 58
\BOOKMARK [1][-]{section.7.1}{Convlutional layer}{figure.6.7}% 59
\BOOKMARK [2][-]{subsection.7.1.1}{Properties of convolutional layer}{section.7.1}% 60
\BOOKMARK [1][-]{section.7.2}{Modified convolutions}{figure.6.7}% 61
\BOOKMARK [1][-]{section.7.3}{Pooling and unpooling layer}{figure.6.7}% 62
\BOOKMARK [1][-]{section.7.4}{Deconvolutional layer}{figure.6.7}% 63
\BOOKMARK [1][-]{section.7.5}{Flatten layer}{figure.6.7}% 64
\BOOKMARK [1][-]{section.7.6}{Global average pooling layer}{figure.6.7}% 65
\BOOKMARK [1][-]{section.7.7}{Architecture of CNNs}{figure.6.7}% 66
\BOOKMARK [0][-]{figure.7.5}{Reccurent/recursive neural networks \(RNN\)}{}% 67
\BOOKMARK [1][-]{section.8.1}{Recruent layer and recurrent neural network}{figure.7.5}% 68
\BOOKMARK [1][-]{section.8.2}{Bididrectional recurrent neural network\(BRNN\)}{figure.7.5}% 69
\BOOKMARK [1][-]{section.8.3}{Long short-term memory \(LSTM\)}{figure.7.5}% 70
\BOOKMARK [0][-]{figure.8.6}{Unsupervised and generative models}{}% 71
\BOOKMARK [1][-]{section.9.1}{Autoencoder\(AE\)}{figure.8.6}% 72
\BOOKMARK [1][-]{section.9.2}{Variational autoencoder}{figure.8.6}% 73
\BOOKMARK [1][-]{section.9.3}{Generative adversarial network}{figure.8.6}% 74
\BOOKMARK [0][-]{section.9.3}{Popular networks and modules}{}% 75
\BOOKMARK [1][-]{section.10.1}{For image classification}{section.9.3}% 76
\BOOKMARK [1][-]{section.10.2}{For object detection}{section.9.3}% 77
\BOOKMARK [1][-]{section.10.3}{For image segmentation}{section.9.3}% 78
\BOOKMARK [1][-]{section.10.4}{For generative tasks}{section.9.3}% 79
\BOOKMARK [0][-]{section.10.4}{Further topics and outlook}{}% 80
\BOOKMARK [1][-]{section.11.1}{Important but unaddressed topics}{section.10.4}% 81
\BOOKMARK [1][-]{section.11.2}{For object detection}{section.10.4}% 82
