\BOOKMARK [0][-]{Doc-Start}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{What is machine learning}{Doc-Start}% 2
\BOOKMARK [1][-]{section.1.2}{What is deep learning?}{Doc-Start}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{What is a neural network \(NN\)}{section.1.2}% 4
\BOOKMARK [1][-]{section.1.3}{Examples for Deep Learning}{Doc-Start}% 5
\BOOKMARK [0][-]{section.1.3}{Tools for Deep Learning}{}% 6
\BOOKMARK [1][-]{section.2.1}{Software}{section.1.3}% 7
\BOOKMARK [1][-]{section.2.2}{Hardware}{section.1.3}% 8
\BOOKMARK [1][-]{section.2.3}{Datasets}{section.1.3}% 9
\BOOKMARK [0][-]{section.2.3}{Machine learning basics}{}% 10
\BOOKMARK [1][-]{section.3.1}{Linear Algebra}{section.2.3}% 11
\BOOKMARK [1][-]{section.3.2}{Random variable and probability distribution}{section.2.3}% 12
\BOOKMARK [2][-]{subsection.3.2.1}{One random vector}{section.3.2}% 13
\BOOKMARK [2][-]{subsection.3.2.2}{Multiple random vectors}{section.3.2}% 14
\BOOKMARK [2][-]{subsection.3.2.3}{Kernel based density estimation }{section.3.2}% 15
\BOOKMARK [1][-]{section.3.3}{Kullback-Leibler divergence and cross entropy }{section.2.3}% 16
\BOOKMARK [2][-]{subsection.3.3.1}{E3.5 KLD between normal and Laplace distribution }{section.3.3}% 17
\BOOKMARK [1][-]{section.3.4}{Probabilistic framework for machine learning}{section.2.3}% 18
\BOOKMARK [2][-]{subsection.3.4.1}{Role of a NN }{section.3.4}% 19
\BOOKMARK [0][-]{subsection.3.4.1}{Dense Neural Networks }{}% 20
\BOOKMARK [1][-]{section.4.1}{Fully connected neural networks - Neuron}{subsection.3.4.1}% 21
\BOOKMARK [1][-]{section.4.2}{Chapter 4.2 Layer of Nurons}{subsection.3.4.1}% 22
\BOOKMARK [1][-]{section.4.3}{Feedforward neural network}{subsection.3.4.1}% 23
\BOOKMARK [1][-]{section.4.4}{Activation function}{subsection.3.4.1}% 24
\BOOKMARK [2][-]{subsection.4.4.1}{Sigmoid activation function}{section.4.4}% 25
\BOOKMARK [2][-]{subsection.4.4.2}{hyperbolic tangent activation function }{section.4.4}% 26
\BOOKMARK [2][-]{subsection.4.4.3}{rectifier linear unit\(ReLU}{section.4.4}% 27
\BOOKMARK [2][-]{subsection.4.4.4}{Softmax activatoin function\(classification problem\)}{section.4.4}% 28
\BOOKMARK [2][-]{subsection.4.4.5}{Special case c=2, binary classification problem}{section.4.4}% 29
\BOOKMARK [1][-]{section.4.5}{Universal approximation }{subsection.3.4.1}% 30
\BOOKMARK [2][-]{subsection.4.5.1}{E4.3 Regression with 1 hidden layer}{section.4.5}% 31
\BOOKMARK [1][-]{section.4.6}{Loss and cost function}{subsection.3.4.1}% 32
\BOOKMARK [2][-]{subsection.4.6.1}{Regression Problem}{section.4.6}% 33
\BOOKMARK [2][-]{subsection.4.6.2}{Classification}{section.4.6}% 34
\BOOKMARK [2][-]{subsection.4.6.3}{Semantic segmentation }{section.4.6}% 35
\BOOKMARK [1][-]{section.4.7}{Training}{subsection.3.4.1}% 36
\BOOKMARK [2][-]{subsection.4.7.1}{Chainrule of derivative \(back propagation\)}{section.4.7}% 37
\BOOKMARK [1][-]{section.4.8}{4.8 Implementation of DNN's in Python}{subsection.3.4.1}% 38
\BOOKMARK [0][-]{section.4.8}{Advanced optimization techniques}{}% 39
\BOOKMARK [1][-]{section.5.1}{Difficulties in optimization }{section.4.8}% 40
\BOOKMARK [2][-]{subsection.5.1.1}{E5.1: sigmoid vs. ReLU}{section.5.1}% 41
\BOOKMARK [1][-]{section.5.2}{Momentum method}{section.4.8}% 42
\BOOKMARK [2][-]{subsection.5.2.1}{Nesterov Momentum}{section.5.2}% 43
\BOOKMARK [1][-]{section.5.3}{5.3 Learning rate schedule}{section.4.8}% 44
\BOOKMARK [1][-]{section.5.4}{Input and batch normalization}{section.4.8}% 45
\BOOKMARK [2][-]{subsection.5.4.1}{E5.3 A Perceptron}{section.5.4}% 46
