\babel@toc {ngerman}{}
\contentsline {chapter}{\numberline {1}Introduction}{2}%
\contentsline {section}{\numberline {1.1}What is machine learning}{2}%
\contentsline {section}{\numberline {1.2}What is deep learning?}{4}%
\contentsline {subsection}{\numberline {1.2.1}What is a neural network (NN)}{4}%
\contentsline {section}{\numberline {1.3}Examples for Deep Learning}{4}%
\contentsline {chapter}{\numberline {2}Tools for Deep Learning}{6}%
\contentsline {section}{\numberline {2.1}Software}{6}%
\contentsline {section}{\numberline {2.2}Hardware}{6}%
\contentsline {section}{\numberline {2.3}Datasets}{6}%
\contentsline {chapter}{\numberline {3}Machine learning basics}{6}%
\contentsline {section}{\numberline {3.1}Linear Algebra}{6}%
\contentsline {section}{\numberline {3.2}Random variable and probability distribution}{8}%
\contentsline {subsection}{\numberline {3.2.1}One random vector}{8}%
\contentsline {subsection}{\numberline {3.2.2}Multiple random vectors}{10}%
\contentsline {subsection}{\numberline {3.2.3}Kernel based density estimation }{10}%
\contentsline {section}{\numberline {3.3}Kullback-Leibler divergence and cross entropy }{12}%
\contentsline {subsection}{\numberline {3.3.1}E3.5 KLD between normal and Laplace distribution }{14}%
\contentsline {section}{\numberline {3.4}Probabilistic framework for machine learning}{14}%
\contentsline {subsection}{\numberline {3.4.1}Role of a NN }{18}%
\contentsline {chapter}{\numberline {4}Dense Neural Networks }{18}%
\contentsline {section}{\numberline {4.1}Fully connected neural networks - Neuron}{18}%
\contentsline {section}{\numberline {4.2}Chapter 4.2 Layer of Nurons}{20}%
\contentsline {section}{\numberline {4.3}Feedforward neural network}{20}%
\contentsline {section}{\numberline {4.4}Activation function}{22}%
\contentsline {subsection}{\numberline {4.4.1}Sigmoid activation function}{22}%
\contentsline {subsection}{\numberline {4.4.2}hyperbolic tangent activation function }{24}%
\contentsline {subsection}{\numberline {4.4.3}rectifier linear unit(ReLU}{24}%
\contentsline {subsection}{\numberline {4.4.4}Softmax activatoin function(classification problem)}{24}%
\contentsline {subsection}{\numberline {4.4.5}Special case c=2, binary classification problem}{26}%
\contentsline {section}{\numberline {4.5}Universal approximation }{26}%
\contentsline {subsection}{\numberline {4.5.1}E4.3 Regression with 1 hidden layer}{26}%
\contentsline {section}{\numberline {4.6}Loss and cost function}{28}%
\contentsline {subsection}{\numberline {4.6.1}Regression Problem}{28}%
\contentsline {subsection}{\numberline {4.6.2}Classification}{28}%
\contentsline {subsection}{\numberline {4.6.3}Semantic segmentation }{30}%
\contentsline {section}{\numberline {4.7}Training}{30}%
\contentsline {subsection}{\numberline {4.7.1}Chainrule of derivative (back propagation)}{32}%
\contentsline {section}{\numberline {4.8}4.8 Implementation of DNN's in Python}{34}%
\contentsline {chapter}{\numberline {5}Advanced optimization techniques}{34}%
\contentsline {section}{\numberline {5.1}Difficulties in optimization }{34}%
\contentsline {subsection}{\numberline {5.1.1}E5.1: sigmoid vs. ReLU}{36}%
\contentsline {section}{\numberline {5.2}Momentum method}{38}%
\contentsline {subsection}{\numberline {5.2.1}Nesterov Momentum}{38}%
\contentsline {section}{\numberline {5.3}5.3 Learning rate schedule}{38}%
\contentsline {section}{\numberline {5.4}Input and batch normalization}{38}%
\contentsline {subsection}{\numberline {5.4.1}E5.3 A Perceptron}{38}%
\contentsline {subsection}{\numberline {5.4.2}Batch normalization}{40}%
\contentsline {section}{\numberline {5.5}Parameter initialization}{42}%
\contentsline {section}{\numberline {5.6}Improved (network)-model}{42}%
\contentsline {chapter}{\numberline {6}Overfitting and regularization}{44}%
\contentsline {section}{\numberline {6.1}Model capacity and overfitting / underfitting}{44}%
\contentsline {section}{\numberline {6.2}Weight norm penalty}{44}%
\contentsline {section}{\numberline {6.3}Early stopping}{44}%
\contentsline {section}{\numberline {6.4}Data augmentation}{44}%
\contentsline {section}{\numberline {6.5}Ensamble learning}{46}%
\contentsline {section}{\numberline {6.6}Dropout}{46}%
\contentsline {section}{\numberline {6.7}Hyperparameter optimization}{48}%
\contentsline {chapter}{\numberline {7}Convolutional neural networks (CNN)}{50}%
\contentsline {section}{\numberline {7.1}Convlutional layer}{50}%
\contentsline {subsection}{\numberline {7.1.1}Properties of convolutional layer}{52}%
\contentsline {section}{\numberline {7.2}Modified convolutions}{54}%
\contentsline {section}{\numberline {7.3}Pooling and unpooling layer}{54}%
\contentsline {section}{\numberline {7.4}Deconvolutional layer}{56}%
\contentsline {section}{\numberline {7.5}Flatten layer}{56}%
\contentsline {section}{\numberline {7.6}Global average pooling layer}{56}%
\contentsline {section}{\numberline {7.7}Architecture of CNNs}{56}%
\contentsline {chapter}{\numberline {8}Reccurent/recursive neural networks (RNN)}{58}%
\contentsline {section}{\numberline {8.1}Recruent layer and recurrent neural network}{58}%
\contentsline {section}{\numberline {8.2}Bididrectional recurrent neural network(BRNN)}{60}%
\contentsline {section}{\numberline {8.3}Long short-term memory (LSTM)}{62}%
\contentsline {chapter}{\numberline {9}Unsupervised and generative models}{64}%
\contentsline {section}{\numberline {9.1}Autoencoder(AE)}{64}%
