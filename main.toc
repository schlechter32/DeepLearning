\contentsline {chapter}{\numberline {1}Introduction}{3}%
\contentsline {section}{\numberline {1.1}What is machine learning}{3}%
\contentsline {section}{\numberline {1.2}What is deep learning?}{3}%
\babel@toc {ngerman}{}
\contentsline {subsection}{\numberline {1.2.1}What is a neural network (NN)}{5}%
\contentsline {section}{\numberline {1.3}Examples for Deep Learning}{5}%
\contentsline {chapter}{\numberline {2}Tools for Deep Learning}{5}%
\contentsline {section}{\numberline {2.1}Software}{5}%
\contentsline {section}{\numberline {2.2}Hardware}{5}%
\contentsline {section}{\numberline {2.3}Datasets}{5}%
\contentsline {chapter}{\numberline {3}Machine learning basics}{7}%
\contentsline {section}{\numberline {3.1}Linear Algebra}{7}%
\contentsline {section}{\numberline {3.2}Random variable and probability distribution}{7}%
\contentsline {subsection}{\numberline {3.2.1}One random vector}{7}%
\contentsline {subsection}{\numberline {3.2.2}Multiple random vectors}{9}%
\contentsline {subsection}{\numberline {3.2.3}Kernel based density estimation }{11}%
\contentsline {section}{\numberline {3.3}Kullback-Leibler divergence and cross entropy }{11}%
\contentsline {subsection}{\numberline {3.3.1}E3.5 KLD between normal and Laplace distribution }{13}%
\contentsline {section}{\numberline {3.4}Probabilistic framework for machine learning}{13}%
\contentsline {subsection}{\numberline {3.4.1}Role of a NN }{17}%
\contentsline {chapter}{\numberline {4}Dense Neural Networks }{19}%
\contentsline {section}{\numberline {4.1}Fully connected neural networks - Neuron}{19}%
\contentsline {section}{\numberline {4.2}Chapter 4.2 Layer of Nurons}{19}%
\contentsline {section}{\numberline {4.3}Feedforward neural network}{21}%
\contentsline {section}{\numberline {4.4}Activation function}{21}%
\contentsline {subsection}{\numberline {4.4.1}Sigmoid activation function}{23}%
\contentsline {subsection}{\numberline {4.4.2}hyperbolic tangent activation function }{23}%
\contentsline {subsection}{\numberline {4.4.3}rectifier linear unit(ReLU}{25}%
\contentsline {subsection}{\numberline {4.4.4}Softmax activatoin function(classification problem)}{25}%
\contentsline {subsection}{\numberline {4.4.5}Special case c=2, binary classification problem}{25}%
\contentsline {section}{\numberline {4.5}Universal approximation }{25}%
\contentsline {subsection}{\numberline {4.5.1}E4.3 Regression with 1 hidden layer}{27}%
\contentsline {section}{\numberline {4.6}Loss and cost function}{27}%
\contentsline {subsection}{\numberline {4.6.1}Regression Problem}{27}%
\contentsline {subsection}{\numberline {4.6.2}Classification}{29}%
\contentsline {subsection}{\numberline {4.6.3}Semantic segmentation }{29}%
\contentsline {section}{\numberline {4.7}Training}{31}%
\contentsline {subsection}{\numberline {4.7.1}Chainrule of derivative (back propagation)}{31}%
\contentsline {section}{\numberline {4.8}4.8 Implementation of DNN's in Python}{33}%
\contentsline {chapter}{\numberline {5}Advanced optimization techniques}{35}%
\contentsline {section}{\numberline {5.1}Difficulties in optimization }{35}%
\contentsline {subsection}{\numberline {5.1.1}E5.1: sigmoid vs. ReLU}{37}%
\contentsline {section}{\numberline {5.2}Momentum method}{37}%
\contentsline {subsection}{\numberline {5.2.1}Nesterov Momentum}{37}%
\contentsline {section}{\numberline {5.3}5.3 Learning rate schedule}{37}%
\contentsline {section}{\numberline {5.4}Input and batch normalization}{39}%
\contentsline {subsection}{\numberline {5.4.1}E5.3 A Perceptron}{39}%
\contentsline {subsection}{\numberline {5.4.2}Batch normalization}{39}%
\contentsline {section}{\numberline {5.5}Parameter initialization}{41}%
\contentsline {section}{\numberline {5.6}Improved (network)-model}{43}%
\contentsline {chapter}{\numberline {6}Overfitting and regularization}{43}%
\contentsline {section}{\numberline {6.1}Model capacity and overfitting / underfitting}{43}%
\contentsline {section}{\numberline {6.2}Weight norm penalty}{43}%
\contentsline {section}{\numberline {6.3}Early stopping}{45}%
\contentsline {section}{\numberline {6.4}Data augmentation}{45}%
\contentsline {section}{\numberline {6.5}Ensamble learning}{45}%
\contentsline {section}{\numberline {6.6}Dropout}{45}%
\contentsline {section}{\numberline {6.7}Hyperparameter optimization}{47}%
\contentsline {chapter}{\numberline {7}Convolutional neural networks (CNN)}{51}%
\contentsline {section}{\numberline {7.1}Convlutional layer}{51}%
\contentsline {subsection}{\numberline {7.1.1}Properties of convolutional layer}{51}%
