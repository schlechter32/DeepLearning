{\reset@font\mtcSfont\mtc@string\contentsline{section}{\noexpand \leavevmode \numberline {1.1}Activation function}{\reset@font\mtcSfont 1}{section.1.1}}
{\reset@font\mtcSSfont\mtc@string\contentsline{subsection}{\noexpand \leavevmode \numberline {1.1.1}Sigmoid activation function}{\reset@font\mtcSSfont 1}{subsection.1.1.1}}
{\reset@font\mtcSSfont\mtc@string\contentsline{subsection}{\noexpand \leavevmode \numberline {1.1.2}hyperbolic tangent activation function }{\reset@font\mtcSSfont 2}{subsection.1.1.2}}
{\reset@font\mtcSSfont\mtc@string\contentsline{subsection}{\noexpand \leavevmode \numberline {1.1.3}rectifier linear unit(ReLU}{\reset@font\mtcSSfont 3}{subsection.1.1.3}}
{\reset@font\mtcSSfont\mtc@string\contentsline{subsection}{\noexpand \leavevmode \numberline {1.1.4}Softmax activatoin function(classification problem)}{\reset@font\mtcSSfont 3}{subsection.1.1.4}}
{\reset@font\mtcSfont\mtc@string\contentsline{section}{\noexpand \leavevmode \numberline {1.2}Special case c=2, binary classification problem}{\reset@font\mtcSfont 4}{section.1.2}}
{\reset@font\mtcSfont\mtc@string\contentsline{section}{\noexpand \leavevmode \numberline {1.3}Chapter 4.5 Universal approximation }{\reset@font\mtcSfont 4}{section.1.3}}
{\reset@font\mtcSSfont\mtc@string\contentsline{subsection}{\noexpand \leavevmode \numberline {1.3.1}E4.3 Regression with 1 hidden layer}{\reset@font\mtcSSfont 5}{subsection.1.3.1}}
{\reset@font\mtcSfont\mtc@string\contentsline{section}{\noexpand \leavevmode \numberline {1.4}Chapter 4.4 - 4.6 Loss and cost function}{\reset@font\mtcSfont 6}{section.1.4}}
