{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZCM65CBt1CJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "JOgMcEajtkmg"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gesTAALEnS7h"
   },
   "source": [
    "# Introduction to TensorFlow\n",
    "# Notebook - Image Segmentation\n",
    "\n",
    "*Disclosure: This notebook is an adaptation of an official TensorFlow tutorial.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMP7mglMuGT2"
   },
   "source": [
    "This tutorial focuses on the task of image segmentation, using a modified <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a>. The U-Net is a U-shaped Encoder-Decoder Architecture that has become the basis for many subsequent networks.\n",
    "\n",
    "## What is image segmentation?\n",
    "So far you have seen image classification, where the task of the network is to assign a label or class to an input image. However, suppose you want to know where an object is located in the image, the shape of that object, which pixel belongs to which object, etc. In this case you will want to segment the image, i.e., each pixel of the image is given a label. Thus, the task of image segmentation is to train a neural network to output a pixel-wise mask of the image. This helps in understanding the image at a much lower level, i.e., the pixel level. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging to name a few.\n",
    "\n",
    "The dataset that will be used for this tutorial is the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), created by Parkhi *et al*. The dataset consists of images, their corresponding labels, and pixel-wise masks. The masks are basically labels for each pixel. Each pixel is given one of three categories :\n",
    "\n",
    "*   Class 1 : Pixel belonging to the pet.\n",
    "*   Class 2 : Pixel bordering the pet.\n",
    "*   Class 3 : None of the above/ Surrounding pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQmKthrSBCld"
   },
   "outputs": [],
   "source": [
    "!pip install -U tfds-nightly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipywidgets) (7.14.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipywidgets) (4.3.3)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Using cached widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipywidgets) (5.0.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (41.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.6.1)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.4.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.3)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.4)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.3)\n",
      "Requirement already satisfied: parso>=0.7.0 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (19.0.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (227)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.7)\n",
      "Requirement already satisfied: testpath in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: widgetsnbextension, ipywidgets\n",
      "Successfully installed ipywidgets-7.5.1 widgetsnbextension-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nicolas hornek\\sw-workspace\\deeplearning-env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g87--n2AtyO_"
   },
   "outputs": [],
   "source": [
    "# fetch tensorflow_examples model code\n",
    "# !pip install git+https://github.com/tensorflow/examples.git # Installs full TF models and code as tensorflow_examples\n",
    "# from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "# tfds.disable_progress_bar()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## Download the Oxford-IIIT Pets dataset\n",
    "\n",
    "The dataset is already included in TensorFlow datasets, all that is needed to do is download it. The segmentation masks are included in version 3+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40ITeStwDwZb"
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37XnUAXBoI9L"
   },
   "outputs": [],
   "source": [
    "# show dataset info\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "The following code performs a simple augmentation of flipping an image. In addition,  image is normalized to [0,1]. Finally, as mentioned above the pixels in the segmentation mask are labeled either {1, 2, 3}. For the sake of convenience, let's subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FD60EbcAQqov"
   },
   "outputs": [],
   "source": [
    "# Normalize the images and masks\n",
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NPlCnBXQwb1"
   },
   "outputs": [],
   "source": [
    "# Resize, normalize and augment the images (and masks)\n",
    "def load_image_train(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf0S67hJRp3D"
   },
   "outputs": [],
   "source": [
    "# Resize, normalize and augment the test data\n",
    "def load_image_test(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "The dataset already contains the required splits of test and train and so let's continue to use the same split.\n",
    "\n",
    "The dataset API will be covered in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHwj2-8SaQli"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cd53d1e555f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTRAIN_LENGTH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTRAIN_LENGTH\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39fYScNz9lmo"
   },
   "outputs": [],
   "source": [
    "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = dataset['test'].map(load_image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeFwFDN6EVoI"
   },
   "outputs": [],
   "source": [
    "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "Let's take a look at an image example and it's correponding mask from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3N2RPAAW9q4W"
   },
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6u_Rblkteqb"
   },
   "outputs": [],
   "source": [
    "for image, mask in train.take(87):\n",
    "  sample_image, sample_mask = image, mask\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## Define the model\n",
    "The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the [Pix2pix tutorial](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). \n",
    "\n",
    "The reason to output three channels is because there are three possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6iB4iMvMkX9"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4mQle3lthit"
   },
   "source": [
    "As mentioned, the encoder will be a pretrained [MobileNetV2](https://arxiv.org/pdf/1801.04381.pdf) model which is prepared and ready to use in [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liCeLH0ctjq7"
   },
   "outputs": [],
   "source": [
    "# Select a fitting model from tf.keras.applications\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Redefine the feature extraction model\n",
    "# Defines multiple layers as ouputs to support skip connections\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers, name='MobileNetV2')\n",
    "\n",
    "down_stack.trainable = False  # Encoder weights are fixed during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPw8Lzra5_T9"
   },
   "source": [
    "The decoder/upsampler is simply a series of upsample blocks implemented in below. The upsample block consists of a transposed convolution (deconvolution), a batch normalization and a ReLU activation. A scheme typically employed in segmentation decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ko0sw4_mfJUN"
   },
   "outputs": [],
   "source": [
    "# Simplified upsample block of pix2pix\n",
    "def upsample(filters, size, name=''):\n",
    "  \"\"\"Upsamples an input.\n",
    "  Conv2DTranspose => Batchnorm => Relu\n",
    "  Args:\n",
    "    filters: number of filters\n",
    "    size: filter size\n",
    "    name: name\n",
    "  Returns:\n",
    "    Upsample Sequential Model\n",
    "  \"\"\"\n",
    "\n",
    "  result = tf.keras.Sequential(name=name)\n",
    "\n",
    "  # Transposed Conv\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                                      use_bias=False))\n",
    "\n",
    "  # add a batch normalization\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  # ReLU activation\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdZ4sx_WqGcd"
   },
   "source": [
    "Define an encoder, consisting of 4 upsampling blocks with 512, 256, 218 and 64 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0ZbfywEbZpJ"
   },
   "outputs": [],
   "source": [
    "# Note: passed names have to be unique\n",
    "up_stack = [\n",
    "    upsample(512, 3, name='upsample_block_1'),  # 4x4 -> 8x8\n",
    "    upsample(256, 3, name='upsample_block_2'),  # 8x8 -> 16x16\n",
    "    upsample(128, 3, name='upsample_block_3'),  # 16x16 -> 32x32\n",
    "    upsample(64, 3, name='upsample_block_4'),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSqOXvNlqt6T"
   },
   "source": [
    "Connect the layer objects to their respecitve encoder inputs and instantiate the whole UNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45HByxpVtrPF"
   },
   "outputs": [],
   "source": [
    "# Use the functional API to connect all skip connections\n",
    "def unet_model(output_channels):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3], name='UNet_input')\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # Add a transpose conv as the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='same',\n",
    "      name='upsampling_final')  #64x64 -> 128x128\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x, name='Modified U-Net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## Train the model\n",
    "Now, all that is left to do is to compile and train the model. The loss being used here is `losses.SparseCategoricalCrossentropy(from_logits=True)`. The reason to use this loss function is because the network is trying to assign each pixel a label, just like multi-class prediction. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class, and `losses.SparseCategoricalCrossentropy(from_logits=True)` is the recommended loss for \n",
    "such a scenario. Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6he36HK5uKAc"
   },
   "outputs": [],
   "source": [
    "# instantiate and compile the model\n",
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVMzbIZLcyEF"
   },
   "source": [
    "Have a quick look at the resulting model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw82qF1Gcovr"
   },
   "outputs": [],
   "source": [
    "# graphical visualization\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ci4pqtAWg_58"
   },
   "outputs": [],
   "source": [
    "# list summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tc3MiEO2twLS"
   },
   "source": [
    "Let's try out the model to see what it predicts before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwvIKLZPtxV_"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLNsrynNtx4d"
   },
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=87):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_1CC0T4dho3"
   },
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22AyVYWQdkgk"
   },
   "source": [
    "Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHrHsqijdmL6"
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StKDH_B9t4SD"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_dataset,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_mu0SAbt40Q"
   },
   "outputs": [],
   "source": [
    "# Fetch history\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BVXldSo-0mW"
   },
   "source": [
    "Let's make some predictions. In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikrzoG24qwf5"
   },
   "outputs": [],
   "source": [
    "show_predictions(test_dataset, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R24tahEqmSCk"
   },
   "source": [
    "## Next steps\n",
    "Now that you have an understanding of what image segmentation is and how it works, you can try this tutorial out with different intermediate layer outputs, or even different pretrained model. You may also want to see the [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) for another model you can retrain on your own data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5W37geGm9Zr"
   },
   "source": [
    "For more use-cases, e.g. examples of how to use callbacks and metrics, have a look at https://www.tensorflow.org/tutorials/keras/regression\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cZCM65CBt1CJ"
   ],
   "name": "2005_intro_nb_segmentation.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb",
     "timestamp": 1588935056602
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
